# Evaluation Results

> Notes:
> - This table is generated by `scripts/evaluate.py` and can be regenerated locally.
> - Metrics used: `cosine_similarity`, `citation_coverage`, `answer_has_citation`.

| id | cosine_similarity | citation_coverage | answer_has_citation | model | notes |
| --- | ---: | ---: | ---: | --- | --- |
| q1 | 0.872 | 1.000 | 1.000 | llama3.2:3b | ok |
| q2 | 0.914 | 1.000 | 1.000 | llama3.2:3b | ok |
| q3 | 0.842 | 1.000 | 1.000 | llama3.2:3b | ok |
| q4 | 0.963 | 1.000 | 1.000 | llama3.2:3b | ok |
| q5 | 0.957 | 1.000 | 1.000 | llama3.2:3b | ok |
| q6 | 0.881 | 1.000 | 1.000 | llama3.2:3b | ok |

## Aggregate

- Mean cosine similarity: **0.905**
- Mean citation coverage: **1.000**
- Mean answer citation presence: **1.000**

## Commentary

- Retrieval grounding is strong for this small corpus setting.
- Slightly lower cosine on conceptual questions suggests room for stronger generation models or reranking.
- Citation formatting remained consistent in all evaluated answers.
