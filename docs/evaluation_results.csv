sample_id,question,eval_session_id,rewritten_question,model,latency_ms,cosine_similarity,citation_coverage,source_precision,source_f1,answer_has_citation,required_term_coverage,rewrite_term_coverage,idk_correct,notes
q1,Why do the authors remove recurrence and convolution in the Transformer?,,Why do the authors remove recurrence and convolution in the Transformer?,llama3.2:1b,15257.97,0.4921,0.0000,0.0000,0.0000,1.0000,0.5000,,,partial_expected_source_coverage;missing_required_terms
q2,"What scaling factor is used in scaled dot-product attention, and why?",,"What scaling factor is used in scaled dot-product attention, and why?",llama3.2:1b,12195.82,0.5846,1.0000,1.0000,1.0000,1.0000,0.0000,,,missing_required_terms
q3,How does the computational complexity per layer of self-attention compare with recurrent layers?,,How does the computational complexity per layer of self-attention compare with recurrent layers?,llama3.2:1b,34675.57,0.8312,1.0000,0.5000,0.6667,1.0000,0.0000,,,missing_required_terms
q4,How many heads are used in base Transformer multi-head attention?,,How many heads are used in base Transformer multi-head attention?,llama3.2:1b,18255.64,0.7211,1.0000,0.3333,0.5000,1.0000,,,,ok
q5,What value of label smoothing is used during training?,,What value of label smoothing is used during training?,llama3.2:1b,16147.51,0.1585,1.0000,0.5000,0.6667,0.0000,,,,missing_answer_citation
q6,What optimizer and learning-rate schedule are used?,,What optimizer and learning-rate schedule are used?,llama3.2:1b,30608.24,0.7869,1.0000,0.5000,0.6667,1.0000,,,,ok
q7,What is scaled dot-product attention?,eval-scaled-dot-product-followup,Why is Scaled Dot-Product Attention (SDPA) scaled?,llama3.2:1b,118246.86,0.8163,1.0000,1.0000,1.0000,1.0000,0.6667,,,missing_required_terms
q8,Why is it scaled?,eval-scaled-dot-product-followup,Turn 2 user: Why is Scaled Dot-Product Attention (SDPA) scaled?,llama3.2:1b,72793.89,0.6629,1.0000,1.0000,1.0000,1.0000,0.5000,1.0000,,missing_required_terms
q9,What GPU memory size was used to pretrain Llama 3 in this document?,,What GPU memory size was used to pretrain Llama 3 in this document?,llama3.2:1b,52357.34,0.2416,1.0000,0.0000,0.0000,1.0000,,,0.0000,idk_mismatch
