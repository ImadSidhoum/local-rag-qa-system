sample_id,question,eval_session_id,rewritten_question,model,latency_ms,cosine_similarity,citation_coverage,source_precision,source_f1,answer_has_citation,required_term_coverage,rewrite_term_coverage,idk_correct,notes
q1,Why do the authors remove recurrence and convolution in the Transformer?,,Why do the authors remove recurrence and convolution in the Transformer?,llama3.2:1b,15715.98,0.5909,1.0000,0.5000,0.6667,1.0000,0.5000,,,missing_required_terms
q2,"What scaling factor is used in scaled dot-product attention, and why?",,"What scaling factor is used in scaled dot-product attention, and why?",llama3.2:1b,9284.23,0.6354,1.0000,1.0000,1.0000,1.0000,0.0000,,,missing_required_terms
q3,How does the computational complexity per layer of self-attention compare with recurrent layers?,,How does the computational complexity per layer of self-attention compare with recurrent layers?,llama3.2:1b,13513.37,0.8230,1.0000,0.3333,0.5000,1.0000,0.0000,,,missing_required_terms
q4,How many heads are used in base Transformer multi-head attention?,,How many heads are used in base Transformer multi-head attention?,llama3.2:1b,12573.45,0.5879,1.0000,0.3333,0.5000,1.0000,,,,ok
q5,What value of label smoothing is used during training?,,What value of label smoothing is used during training?,llama3.2:1b,7229.07,0.3446,1.0000,0.3333,0.5000,0.0000,,,,missing_answer_citation
q6,What optimizer and learning-rate schedule are used?,,What optimizer and learning-rate schedule are used?,llama3.2:1b,12503.53,0.7194,1.0000,0.5000,0.6667,1.0000,,,,ok
q7,What is scaled dot-product attention?,eval-01daf70d-scaled-dot-product-followup,What is scaled dot-product attention?,llama3.2:1b,13443.46,0.6345,1.0000,1.0000,1.0000,1.0000,1.0000,,,ok
q8,Why is it scaled?,eval-01daf70d-scaled-dot-product-followup,Why does Multi-Head Attention require scaling?,llama3.2:1b,18634.05,0.1992,1.0000,0.5000,0.6667,1.0000,0.0000,0.0000,,missing_required_terms;rewrite_terms_not_fully_matched
q9,What GPU memory size was used to pretrain Llama 3 in this document?,,What GPU memory size was used to pretrain Llama 3 in this document?,llama3.2:1b,8036.67,1.0000,1.0000,0.0000,0.0000,0.0000,,,1.0000,missing_answer_citation
