[
  {
    "id": "q1",
    "question": "Why do the authors remove recurrence and convolution in the Transformer?",
    "expected_answer": "The paper argues recurrence limits parallelization and increases path length. Self-attention enables more parallel computation and shorter paths between dependencies.",
    "expected_sources": [{"source": "attention_is_all_you_need.pdf", "page": 1}]
  },
  {
    "id": "q2",
    "question": "What scaling factor is used in scaled dot-product attention, and why?",
    "expected_answer": "The dot product is scaled by 1/sqrt(d_k) to prevent large magnitude values that push softmax into regions with tiny gradients.",
    "expected_sources": [{"source": "attention_is_all_you_need.pdf", "page": 4}]
  },
  {
    "id": "q3",
    "question": "How does the computational complexity per layer of self-attention compare with recurrent layers?",
    "expected_answer": "Self-attention has O(n^2 * d) complexity per layer while recurrent layers have O(n * d^2), with trade-offs depending on sequence length and representation dimension.",
    "expected_sources": [{"source": "attention_is_all_you_need.pdf", "page": 6}]
  },
  {
    "id": "q4",
    "question": "How many heads are used in base Transformer multi-head attention?",
    "expected_answer": "The base model uses 8 attention heads.",
    "expected_sources": [{"source": "attention_is_all_you_need.pdf", "page": 5}]
  },
  {
    "id": "q5",
    "question": "What value of label smoothing is used during training?",
    "expected_answer": "The paper uses label smoothing with epsilon_ls = 0.1.",
    "expected_sources": [{"source": "attention_is_all_you_need.pdf", "page": 7}]
  },
  {
    "id": "q6",
    "question": "What optimizer and learning-rate schedule are used?",
    "expected_answer": "They use Adam with beta1=0.9, beta2=0.98, epsilon=1e-9, and a warmup/inverse-square-root learning-rate schedule with 4000 warmup steps.",
    "expected_sources": [{"source": "attention_is_all_you_need.pdf", "page": 7}]
  }
]
